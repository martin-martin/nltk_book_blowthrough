{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A world of Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg as gb\n",
    "gb.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      ". She was not struck by any thing remarkably clever in Miss Smith ' s conversa\n",
      "He is very plain , undoubtedly -- remarkably plain :-- but that is nothing com\n",
      "!\" \" Mr . Knightley ' s air is so remarkably good that it is not fair to compa\n",
      " before of Mr . Elton ' s being a remarkably handsome man , with most agreeabl\n",
      "lever boy , indeed . They are all remarkably clever ; and they have so many pr\n",
      "quiet manners , and a disposition remarkably amiable and affectionate ; wrapt \n",
      " my children in : but _we_ are so remarkably airy !-- Mr . Wingfield thinks th\n",
      "ut for her private perplexities , remarkably comfortable , as such seclusion e\n",
      ". Jane Fairfax was very elegant , remarkably elegant ; and she had herself the\n",
      " Very nicely dressed , indeed ; a remarkably elegant gown .\" \" I am not at all\n",
      " am --.' Thank you , my mother is remarkably well . Gone to Mr . Woodhouse ' s\n"
     ]
    }
   ],
   "source": [
    "emma = nltk.Text(gb.words('austen-emma.txt'))\n",
    "emma.concordance(\"remarkably\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I knew about that. Let's try importing a text to perform concordance search on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics (importing text files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use pathlib to easily import a file\n",
    "raw_lyrics = Path(\"resources/lottalyrics.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the text needs to be word-tokenized\n",
    "tok_lyrics = word_tokenize(raw_lyrics)\n",
    "# for creating a nltk.Text object\n",
    "lyrics = nltk.Text(tok_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "on nalga sonando aplaudiendo con el trasero parece q me est√°n hablando Y me gus\n",
      "e panadero Barriendo el piso con el trasero Toda la grasa se desplaza por la te\n"
     ]
    }
   ],
   "source": [
    "lyrics.concordance(\"trasero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web and Chat Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set remove ...\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Who ...\n",
      "overheard.txt White guy: So, do you have any plans for this even ...\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted ...\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for  ...\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished lea ...\n"
     ]
    }
   ],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:50], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830118"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc = webtext.raw(\"overheard.txt\")\n",
    "len(nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guy #1: So this Jack guy is basically the luckiest man in the world.\n",
      "Guy #2: Why\n"
     ]
    }
   ],
   "source": [
    "print(nyc[120:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-19-20s_706posts.xml',\n",
       " '10-19-30s_705posts.xml',\n",
       " '10-19-40s_686posts.xml',\n",
       " '10-19-adults_706posts.xml',\n",
       " '10-24-40s_706posts.xml',\n",
       " '10-26-teens_706posts.xml',\n",
       " '11-06-adults_706posts.xml',\n",
       " '11-08-20s_705posts.xml',\n",
       " '11-08-40s_706posts.xml',\n",
       " '11-08-adults_705posts.xml',\n",
       " '11-08-teens_706posts.xml',\n",
       " '11-09-20s_706posts.xml',\n",
       " '11-09-40s_706posts.xml',\n",
       " '11-09-adults_706posts.xml',\n",
       " '11-09-teens_706posts.xml']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nps_chat.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',', 'I', 'can', 'look', 'in', 'a', 'mirror', '.']\n"
     ]
    }
   ],
   "source": [
    "chatroom = nps_chat.posts(nps_chat.fileids()[0])\n",
    "print(len(chatroom))\n",
    "print(chatroom[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# made in 1961 at Brown university (near Boston)\n",
    "from nltk.corpus import brown\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Office', 'of', 'Business', 'Economics', '(', 'OBE', ')', 'of', 'the', 'U.S.', 'Department', 'of', 'Commerce', 'provides', 'basic', 'measures', 'of', 'the', 'national', 'economy', 'and', 'current', 'analysis', 'of', 'short-run', 'changes', 'in', 'the', 'economic', 'situation', 'and', 'business', 'outlook', '.'], ['It', 'develops', 'and', 'analyzes', 'the', 'national', 'income', ',', 'balance', 'of', 'international', 'payments', ',', 'and', 'many', 'other', 'business', 'indicators', '.'], ['Such', 'measures', 'are', 'essential', 'to', 'its', 'job', 'of', 'presenting', 'business', 'and', 'Government', 'with', 'the', 'facts', 'required', 'to', 'meet', 'the', 'objective', 'of', 'expanding', 'business', 'and', 'improving', 'the', 'operation', 'of', 'the', 'economy', '.'], ['Contact'], ['For', 'further', 'information', 'contact', 'Director', ',', 'Office', 'of', 'Business', 'Economics', ',', 'U.S.', 'Department', 'of', 'Commerce', ',', 'Washington', '25', ',', 'D.C.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# access by words or sents, optionally with a specific ID\n",
    "#text = brown.words(categories='government')\n",
    "text = brown.sents(categories='government')\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(brown.sents()[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the \"*ing\" words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ing(genre_list):\n",
    "    \"\"\"lists all high-frequency words ending with -ing in the various genres of the Brown corpus.\"\"\"\n",
    "    frequency = 20\n",
    "    for genre in genre_list:\n",
    "        genre_words = brown.words(categories=genre)\n",
    "        fdist = nltk.FreqDist(w for w in genre_words)\n",
    "        th_ings = set([w for w in genre_words if w.endswith(\"ing\") and fdist[w] >= frequency])\n",
    "        print(genre, \"->\")\n",
    "        for ing in th_ings:\n",
    "            print(\"{}: {}\".format(ing, fdist[ing]), end=\", \")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure ->\n",
      "nothing: 38, running: 21, morning: 27, thinking: 20, thing: 28, anything: 28, going: 50, something: 50, \n",
      "\n",
      "belles_lettres ->\n",
      "being: 136, bring: 22, feeling: 35, according: 24, During: 21, understanding: 42, evening: 22, during: 74, living: 37, writing: 57, going: 33, reading: 28, something: 49, nothing: 77, thinking: 23, thing: 44, beginning: 35, having: 59, anything: 38, doing: 23, following: 29, looking: 21, morning: 27, trying: 24, including: 22, coming: 25, making: 39, \n",
      "\n",
      "editorial ->\n",
      "being: 45, during: 25, going: 25, \n",
      "\n",
      "fiction ->\n",
      "being: 31, nothing: 26, morning: 32, getting: 21, thing: 30, anything: 26, going: 45, coming: 21, something: 45, \n",
      "\n",
      "government ->\n",
      "operating: 20, being: 42, during: 51, financing: 21, During: 29, hearing: 27, including: 25, planning: 36, making: 25, \n",
      "\n",
      "hobbies ->\n",
      "working: 24, being: 36, cooling: 30, during: 31, marketing: 30, training: 25, locking: 25, building: 26, using: 22, shooting: 29, \n",
      "\n",
      "humor ->\n",
      "\n",
      "\n",
      "learned ->\n",
      "being: 119, coating: 33, feeling: 23, according: 23, During: 32, cutting: 29, meeting: 22, meaning: 41, during: 89, containing: 27, training: 54, using: 43, increasing: 25, reading: 26, washing: 29, something: 38, operating: 38, nothing: 38, thing: 29, beginning: 25, having: 37, sampling: 20, corresponding: 25, taking: 20, resulting: 21, following: 70, staining: 36, including: 32, making: 30, \n",
      "\n",
      "lore ->\n",
      "nothing: 29, being: 52, bring: 22, during: 55, thing: 25, having: 28, anything: 23, reading: 22, taking: 20, something: 21, making: 31, \n",
      "\n",
      "mystery ->\n",
      "nothing: 30, looking: 24, morning: 24, building: 21, thing: 31, anything: 37, going: 66, coming: 22, something: 52, waiting: 23, \n",
      "\n",
      "news ->\n",
      "being: 56, during: 55, playing: 20, according: 23, going: 22, including: 25, meeting: 62, making: 25, \n",
      "\n",
      "religion ->\n",
      "being: 30, \n",
      "\n",
      "reviews ->\n",
      "being: 30, singing: 20, playing: 21, \n",
      "\n",
      "romance ->\n",
      "nothing: 47, being: 49, looking: 36, watching: 20, feeling: 21, morning: 40, thinking: 24, thing: 52, trying: 23, doing: 20, anything: 42, having: 24, going: 60, something: 59, everything: 27, \n",
      "\n",
      "science_fiction ->\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show high-frequency -ing words in the different genres of the Brown corpus\n",
    "get_ing(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat stuff! NLTK's **Conditional Frequency Distributions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "        (genre, word)\n",
    "        for genre in brown.categories()\n",
    "        for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 what   who   why  when where \n",
      "      adventure   110    91    13   126    53 \n",
      " belles_lettres   244   452    36   252   107 \n",
      "      editorial    84   172    10   103    40 \n",
      "        fiction   128   103    18   133    76 \n",
      "     government    43    74     6    56    46 \n",
      "        hobbies    78   103    10   119    72 \n",
      "          humor    36    48     9    52    15 \n",
      "        learned   141   212    20   227   118 \n",
      "           lore   130   259    25   182    97 \n",
      "        mystery   109    80    25   114    59 \n",
      "           news    76   268     9   128    58 \n",
      "       religion    64   100    14    53    20 \n",
      "        reviews    44   128     9    54    25 \n",
      "        romance   121    89    34   126    54 \n",
      "science_fiction    27    13     4    21    10 \n"
     ]
    }
   ],
   "source": [
    "#genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "genres = brown.categories()\n",
    "words = ['what', 'who', 'why', 'when', 'where']\n",
    "cfd.tabulate(conditions=genres, samples=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting in which genres which of these words are more common. Whys are _so rare_! Only belle_lettres, romance, and to a lesser degree mystery, lore and learned seem to be concerned about finding out the reason for things..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "lots of stuff in between\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/target.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lover\n",
    "- Giver\n",
    "- Girl\n",
    "- _Vergil_\n",
    "- rove\n",
    "- _love_\n",
    "- _give_\n",
    "- _go_\n",
    "- _gin_\n",
    "- ren\n",
    "- _live_\n",
    "- liver\n",
    "- virgo\n",
    "- Norge\n",
    "- lore\n",
    "- over\n",
    "- goner\n",
    "- _vive_\n",
    "- _legion_\n",
    "- region\n",
    "\n",
    "(enough for now. not a very good perormance!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:car]",
   "language": "python",
   "name": "conda-env-car-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
